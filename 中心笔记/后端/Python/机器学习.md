## 机器学习和人工智能,深度学习的关系

- 机器学习是人工智能的一个实现途径
    
- 深度学习是机器学习的一个方法(人工神经网络)发展而来
    

## 什么是机器学习
![[Pasted image 20250715135515.png]]
机器学习是从数据中自动分析获得模型，并利用模型对未知数据进行预测。

## 机器学习算法分类
![[Pasted image 20250715135523.png]]
#### 监督学习

定义：输入数据是由输入特征值和目标值所组成。函数的输出可以是一个连续的值（称为回归），或是输出是有限个离散值（称作分类）

**常见算法:**

- 分类 k－近邻算法、贝叶斯分类、决策树与随机森林、逻辑回归
    
- 回归 线性回归、岭回归
    

目标值：类别 -- 分类问题(e.g.判别一张图片是猫还是狗)

目标值：连续型的数据 -- 回归问题(根据房屋特征值估算价值)

#### 无监督学习

定义：输入数据是由输入特征值所组成。

**常见算法:**

- 聚类 k－means
    

目标值：无 -- 无监督学习

## 开发流程
![[Pasted image 20250715135533.png]]
![[Pasted image 20250715135537.png]]
1. 获取数据
    
2. 数据处理
    
3. 特征工程
    
4. 机器学习算法训练 - 模型
    
5. 模型评估
    
6. 应用
    

## Scikit-learn数据集

机器学习用到的数据称为数据集
![[Pasted image 20250715135554.png]]
> 每一行数据都可以称为样本
> 
> 有的数据集可以没有目标值(如有时会把特征相似的分为一类,而不是特定目标值,只做分类,不做定义)

#### 安装

```Shell
pip3 install Scikit-learn
# 安装完成后可以使用import sklearn测试
```

#### 使用

1. API
    

- sklearn.datasets
    
    - 加载获取流行数据集
        
    - datasets.load_*() (*是数据集名称)
        
        - 获取小规模数据集，数据包含在datasets里
            
        - E.G. sklearn.datasets.load_boston() 加载并返回波士顿房价数据集
            
    - datasets.fetch_*(data_home=None,subset='train')
        
        - 获取大规模数据集，需要从网络上下载，函数的第一个参数是data_home，表示数据集下载的目录，默认是是~/scikit_learn_data/.第二个参数subset:'train'或者'test'，'all',可选，选择要加载的数据集。
            

这两个方法返回的数据类型都是datasets.base.Bunch(字典格式)

data：特征数据数组，是[n_samples*n_features]的二维

numpy.ndarray 数组

target：标签数组，是n_samples的一维numpy.ndarray 数组

DESCR：数据描述

feature_names：特征名，新闻数据，手写数字、回归数据集没有

target names：标签名

**实例:**

```Python
from sklearn.datasets import load_iris #加载sk数据集中的鸢尾花数据集

def datasets_demo():
    """
    sklearn数据集使用
    :return:
    """
    iris = load_iris()
    print("鸢尾花数据集:In", iris)
    print("查看数据集描述:\n",iris["DESCR"]) #Bunch可以使用字典方式访问
    print("查看特征值的名字:\n", iris.feature_names) #也可以使用.属性名的方式访问
    print("查看特征值:\n", iris.data, iris.data.shape)
    return None

if __name__ == '__main__':
    datasets_demo()
```

#### 数据集的划分

机器学习一般的数据集会划分为两个部分：

- 训练数据：用于训练，构建模型
    
- 测试数据：在模型检验时使用，用于评估模型是否有效
    

可以使用sklearn.model_selection.train_test_split(arrays, *options)来划分

- χ数据集的特征值
    
- y数据集的标签值
    
- test_size测试集的大小,一般为float
    
- random_state随机数种子，不同的种子会造成不同的随机采样结果。相同的种子采样结果相同。
    
- return训练集特征值，测试集特征值，训练集目标值，测试集目标值
    

```Python
from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2, random_state=22) (后两个参数可以不填,默认0.25)
print("训练集的特征值:\n", x_train, x_train.shape)
```

## 特征提取

特征工程是使用专业背景知识和技巧处理数据，使得特征能在机器学习算法上发挥更好的作用的过程。

#### 字典特征提取

将任意数据（如文本或图像）转换为可用于机器学习的数字特征

可以用 sklearn.feature_extraction 来进行

sklearn.feature_extraction.DictVectorizer(sparse=True,..)用于转化字典,它会把字典中的类别转为one-hot编码的形式(默认传参会把返回值化为稀疏矩阵,改为false即可还原为二维数组,或者调用稀疏矩阵的toarray转化为二维矩阵)

> 稀疏矩阵就是二维矩阵中的坐标加值的形式,去除了其中的0
> 
> [[ 0. 1. 0. 100.]
> 
> [ 1. 0. 0. 60.]
> 
> [ 0. 0. 1. 30.]]
> 
> 稀疏矩阵
> 
> Coords Values
> 
> (0, 1) 1.0
> 
> (0, 3) 100.0
> 
> (1, 0) 1.0
> 
> (1, 3) 60.0
> 
> (2, 2) 1.0
> 
> (2, 3) 30.0

> one-hot编码就是有几个类别就有几列,因为这样表示每一个类别的都是1,如果用不同的数直接表示,如苹果是1,香蕉是2,可能会导致认为香蕉在某方面优于苹果

- DictVectorizer.fit_transform(X)X:字典或者包含字典的迭代器返回值：返回sparse矩阵(系数矩阵)
    
- DictVectorizer.inverse_transform(X) X:array数组或者sparse矩阵返回值：转换之前数据格式
    
- DictVectorizer.get_feature_names()返回类别名称,即每一列的属性名
![[Pasted image 20250715135616.png]]应用场景：

1. 数据集中类别特征比较多时,如类别特征为各种水果名
    
    1. 先将数据集的特征 -》字典类型
        
    2. DictVectorizer转换
        
2. 本身拿到的数据就是字典类型
    

#### 文本特征提取

> 处理文本时会将标点符号和单个字母去掉,不计入统计

1. **CountVectorizer**
    

- sklearn.feature_extraction.text.CountVectorizer(stop_words=[])
    
    - stop_words是停用词,即不计入统计的单词
        
    - 返回词频矩阵(统计每个单词出现的次数)
        
- CountVectorizer.fit_transform(X)
    
    - X是文本或者包含文本字符串的可迭代对象
        
    - 返回值：返回sparse矩阵
        
- CountVectorizer.inverse_transform(X)
    
    - X:array数组或者sparse矩阵
        
    - 返回值:转换之前数据格
        
- CountVectorizer.get_feature_names()
    
    - 返回值:单词列表
        
- sklearn.feature_extraction.text.TfidfVectorizer
    

对于中文文本,因为不会像英文每个单词之间有空格分开,直接提取会拿标点符号分词,所以需要自己分词

#### Jieba

**安装:****`pip install jieba`**

可以使用jieba.cut(str)返回一个词语组成的生成器,这个生成器可以强转为list,就可以得到str分词后的list,要用于提取还需要用空格拼接一下

```Python
data = ["我爱北京天安门", "天安门上太阳升"]
data_new = []
for text in data:
    data_new.append(" ".join(list(jieba.cut(text)))) #是字符串,不是list
```

2. **TfidfVevtorizer**
    

TF-IDF的主要思想是：如果某个词或短语在一篇文章中出现的概率高，并且在其他文章中很少出现，则认为此词或者短语具有很好的类别区分能力，适合用来分类。

TF-IDF作用：用以评估一字词对于一个文件集或一个语料库中的其中一份文件的重要程度。

计算公式:
![[Pasted image 20250715135629.png]]
使用:

- sklearn.feature_extraction.text.TfidfVectorizer(stop_words=None,..)
    
    - 返回词的权重矩阵
        
- TfidfVectorizer.fit_transform(X)
    
    - X:文本或者包含文本字符串的可迭代对象
        
    - 返回值：返回sparse矩阵
        
- TfidfVectorizer.inverse_transform(X)
    
    - X:array数组或者sparse矩阵
        
    - 返回值:转换之前数据格式
        
- TfidfVectorizer.get_feature_names0
    
    - 返回值：单词列表
        

## 特征预处理

通过一些转换函数将特征数据转换成更加适合算法模型的特征数据过程

- 数值型数据的无量纲化：
    
    - 归一化
        
    - 标准化
        

为什么我们要进行归一化/标准化?

特征的单位或者大小相差较大，或者某特征的方差相比其他的特征要大出几个数量级，容易影响（支配）目标结果，使得一些算法无法学习到其它的特征.如月收入和年龄,前者可能是上万的,后者则不会过百,此时在计算两个样本点之间的距离时(类似几何坐标点距离的计算)就会使收入决定结果,年龄的作用微乎其微,所以需要进行无量纲化,使不同规格的数据转化为统一规格

#### 归一化
![[Pasted image 20250715135643.png]]
API:

- sklearn.preprocessing.MinMaxScaler (feature_range=[0,1]...) 实例化缩放器
    
- MinMaxScalar.fit_tyansform(X) 缩放
    
    - X:numpy_array格式的数据[n_samples,n_features](就是一个数组)
        
    - 返回值：转换后的形状相同的array
        

注意最大值最小值是变化的，另外，最大值与最小值非常容易受异常点影响，所以这种方法鲁棒性较差，只适合传统精确小数据场景。

#### Pandas

读取本地文件可以使用pandas.read_csv("<文件名>"),返回后的数据如果有不需要的列或行可以调用返回值的.iloc来截取,用法类似正常截取

```Python
improt pandas

data = pandas.read_csv("dating.txt")
data = data.iloc[:, :3] #获取所有行,前三列
print(data)
```

#### 标准化
![[Pasted image 20250715135653.png]]
对于归一化来说：如果出现异常点，影响了最大值和最小值，那么结果显然会发生改变

对于标准化来说：如果出现异常点，由于具有一定数据量，少量的异常点对于平均值的影响并不大，从而方差改变较小。

**API:**

- sklearn.preprocessing.StandardScaler() 实例化缩放器
    
    - 处理之后，对每列来说，所有数据都聚集在均值为0附近，标准差为1
        
- StandardScaler.fit_transform(X) 标准化
    
    - X:numpy array格式的数据[n_samples,n_features]
        
    - 返回值：转换后的形状相同的array
        

在已有样本足够多的情况下比较稳定，适合现代嘈杂大数据场景。

## 特征降维
![[Pasted image 20250715135702.png]]
#### 特征选择

数据中包含冗余或相关变量（或称特征、属性、指标等），旨在从原有特征中找出主要特征。
![[Pasted image 20250715135713.png]]
**过滤式:**

1. 低方差特征过滤
    

删除低方差的一些特征。

特征方差小：某个特征大多样本的值比较相近

特征方差大：某个特征很多样本的值都有差别

API:

- sklearn.feature_selection.VarianceThreshold(threshold = 0.0)
    
    - 删除所有低方差特征
        
- Variance.fit_transform(X)
    
    - X:numpy array格式的数据[n_samples,n_features]
        
    - 返回值：训练集差异低于threshold的特征将被删除。默认值是保留所有非零方差特征，即删除所有样本中具有相同值的特征。